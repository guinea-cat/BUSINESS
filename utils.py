"""
utils.py
åŠŸèƒ½å·¥å…·é›†ï¼Œæä¾› PDF è§£æã€ç½‘ç»œæœç´¢åŠæ•°æ®æ¸…æ´—åŠŸèƒ½ã€‚
ä¼˜åŒ–ç‰ˆï¼šå¼•å…¥å¹¶å‘æœºåˆ¶å’Œå›¾ç‰‡æ‹¼æ¥ç­–ç•¥ï¼Œå¤§å¹…æå‡åˆ†æé€Ÿåº¦ã€‚
"""

import re
import json
import logging
import requests
import fitz  # PyMuPDF
import io
import base64
from PIL import Image
from typing import List, Optional, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def extract_content_from_pdf(pdf_path: str) -> Dict[str, any]:
    """
    ä½¿ç”¨ PyMuPDF ä» PDF ä¸­æå–æ–‡æœ¬å’Œå›¾åƒå†…å®¹ã€‚
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. è¿‡æ»¤å¼‚å¸¸é•¿å®½æ¯”å›¾ç‰‡ï¼ˆé¡µçœ‰é¡µè„šçº¿æ¡ï¼‰
    2. æŒ‰æ–‡ä»¶ä½“ç§¯æ’åºï¼Œä¼˜å…ˆåˆ†æå¤æ‚å›¾è¡¨
    3. é™åˆ¶æœ€å¤§åˆ†ææ•°é‡ä¸º 50 å¼ 
    """
    full_text = ""
    images_with_size = []  # å­˜å‚¨ (å›¾ç‰‡æ•°æ®, æ–‡ä»¶ä½“ç§¯) å…ƒç»„
    
    try:
        doc = fitz.open(pdf_path)
        for page_index in range(len(doc)):
            page = doc[page_index]
            
            # 1. æå–æ–‡æœ¬
            full_text += page.get_text() + "\n"
            
            # 2. æå–å›¾åƒ
            image_list = page.get_images(full=True)
            for img_index, img in enumerate(image_list):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                
                # è¿‡æ»¤å¤ªå°çš„å›¾æ ‡ç±»å›¾ç‰‡ (ä¾‹å¦‚å°äº 5KB)
                if len(image_bytes) < 5000:
                    continue
                
                # --- æ–°å¢ï¼šè¿‡æ»¤å¼‚å¸¸é•¿å®½æ¯”çš„å›¾ç‰‡ ---
                try:
                    img_obj = Image.open(io.BytesIO(image_bytes))
                    aspect_ratio = img_obj.width / img_obj.height if img_obj.height > 0 else 0
                    
                    # è¿‡æ»¤é•¿å®½æ¯” > 5:1 æˆ– < 1:5 çš„å›¾ç‰‡ï¼ˆé€šå¸¸æ˜¯é¡µçœ‰é¡µè„šçº¿æ¡ï¼‰
                    if aspect_ratio > 5 or aspect_ratio < 0.2:
                        logger.debug(f"è·³è¿‡å¼‚å¸¸é•¿å®½æ¯”å›¾ç‰‡: {aspect_ratio:.2f}")
                        continue
                    
                    # ç¼©æ”¾ä¸å‹ç¼©
                    if img_obj.width > 1024:
                        ratio = 1024 / img_obj.width
                        new_size = (1024, int(img_obj.height * ratio))
                        img_obj = img_obj.resize(new_size, Image.Resampling.LANCZOS)
                    
                    if img_obj.mode in ("RGBA", "P"):
                        img_obj = img_obj.convert("RGB")
                    
                    buffer = io.BytesIO()
                    img_obj.save(buffer, format="JPEG", quality=75)
                    image_bytes = buffer.getvalue()
                    img_ext = "jpg"
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œä¿ç•™åŸå›¾: {e}")
                    img_ext = base_image["ext"]

                # å­˜å‚¨å›¾ç‰‡åŠå…¶ä½“ç§¯ï¼ˆç”¨äºåç»­æ’åºï¼‰
                base64_img = base64.b64encode(image_bytes).decode('utf-8')
                images_with_size.append({
                    "page": page_index + 1,
                    "data": base64_img,
                    "ext": img_ext,
                    "size": len(image_bytes)
                })
        
        doc.close()
        
        # --- æŒ‰ä½“ç§¯æ’åºï¼Œé€‰å–æœ€å¤§çš„ 50 å¼ ï¼ˆåˆ†ææ‰€æœ‰æœ‰æ•ˆå›¾ç‰‡ï¼‰---
        images_with_size.sort(key=lambda x: x["size"], reverse=True)
        selected_images = [
            {"page": img["page"], "data": img["data"], "ext": img["ext"]}
            for img in images_with_size[:50]  # æœ€å¤šåˆ†æ 50 å¼ 
        ]
        
        logger.info(f"PDF è§£æå®Œæˆï¼šæå– {len(images_with_size)} å¼ æœ‰æ•ˆå›¾ç‰‡ï¼Œå°†åˆ†æ {len(selected_images)} å¼ ")
        
        return {
            "text": full_text.strip(),
            "images": selected_images
        }
    except Exception as e:
        logger.error(f"è§£æ PDF å¤±è´¥: {pdf_path}, é”™è¯¯: {e}")
        return {"text": f"PDF æå–å¤±è´¥: {str(e)}", "images": []}


def stitch_images(images: List[Dict], grid_size: int = 3) -> List[Dict]:
    """
    å°†å›¾ç‰‡æŒ‰ç½‘æ ¼æ‹¼æ¥,å‡å°‘ API è¯·æ±‚æ¬¡æ•°ã€‚
    
    å‚æ•°:
        images: åŸå§‹å›¾ç‰‡åˆ—è¡¨
        grid_size: ç½‘æ ¼å¤§å°ï¼ˆé»˜è®¤ 3x3ï¼Œå³ 9 å¼ æ‹¼æˆ 1 å¼ ï¼‰
    
    è¿”å›:
        æ‹¼æ¥åçš„å›¾ç‰‡åˆ—è¡¨
    """
    stitched_images = []
    batch_size = grid_size * grid_size
    
    for i in range(0, len(images), batch_size):
        batch = images[i:i + batch_size]
        
        if len(batch) == 1:
            # åªæœ‰ 1 å¼ ï¼Œç›´æ¥ä¿ç•™
            stitched_images.append(batch[0])
            continue
        
        try:
            # è§£ç  base64 å›¾ç‰‡
            pil_images = []
            for img in batch:
                img_data = base64.b64decode(img["data"])
                pil_images.append(Image.open(io.BytesIO(img_data)))
            
            # è®¡ç®—æ‹¼æ¥åçš„ç”»å¸ƒå¤§å°
            max_width = max(img.width for img in pil_images)
            max_height = max(img.height for img in pil_images)
            
            # åˆ›å»ºç©ºç™½ç”»å¸ƒ
            canvas_width = max_width * grid_size
            canvas_height = max_height * grid_size
            canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))
            
            # ç²˜è´´å›¾ç‰‡åˆ°ç½‘æ ¼
            for idx, pil_img in enumerate(pil_images):
                row = idx // grid_size
                col = idx % grid_size
                x = col * max_width + (max_width - pil_img.width) // 2
                y = row * max_height + (max_height - pil_img.height) // 2
                canvas.paste(pil_img, (x, y))
            
            # è½¬æ¢å› base64
            buffer = io.BytesIO()
            canvas.save(buffer, format="JPEG", quality=80)
            stitched_data = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            stitched_images.append({
                "data": stitched_data,
                "ext": "jpg",
                "pages": [img["page"] for img in batch],
                "count": len(batch)
            })
            
        except Exception as e:
            logger.warning(f"æ‹¼æ¥å›¾ç‰‡å¤±è´¥ï¼Œä¿ç•™åŸå›¾: {e}")
            stitched_images.extend(batch)
    
    logger.info(f"å›¾ç‰‡æ‹¼æ¥å®Œæˆï¼š{len(images)} å¼  â†’ {len(stitched_images)} å¼ ï¼ˆå‡å°‘ {len(images) - len(stitched_images)} æ¬¡è¯·æ±‚ï¼‰")
    return stitched_images


def _analyze_single_image(client, img: Dict, index: int) -> tuple:
    """
    åˆ†æå•å¼ å›¾ç‰‡çš„å·¥ä½œå‡½æ•°ï¼ˆç”¨äºå¹¶å‘æ‰§è¡Œï¼‰ã€‚
    æ”¯æŒæ‹¼æ¥å›¾å’Œæ™®é€šå›¾ä¸¤ç§æ¨¡å¼ã€‚
    
    è¿”å›: (index, page_info, description) å…ƒç»„
    """
    try:
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ‹¼æ¥å›¾ï¼ˆç”± stitch_images å‡½æ•°æ·»åŠ  count å­—æ®µï¼‰
        if "count" in img:
            prompt = f"è¿™æ˜¯ {img['count']} å¼ å•†ä¸šè®¡åˆ’ä¹¦å›¾ç‰‡çš„æ‹¼è´´ï¼ˆæŒ‰ 3x3 ç½‘æ ¼æ’åˆ—ï¼‰ã€‚è¯·ä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹é€ä¸ªæè¿°æ¯å¼ å›¾çš„æ ¸å¿ƒå†…å®¹ï¼ˆå¦‚æ•°æ®å›¾è¡¨ã€å•†ä¸šæ¨¡å¼å›¾ã€äº§å“åŸå‹æˆ–è´¢åŠ¡é¢„æµ‹ï¼‰ã€‚"
            page_info = f"æ‹¼è´´å›¾ (ç¬¬ {', '.join(map(str, img['pages']))} é¡µ)"
        else:
            prompt = "è¿™æ˜¯ä¸€å¼ å•†ä¸šè®¡åˆ’ä¹¦ï¼ˆBPï¼‰ä¸­çš„å›¾ç‰‡ï¼Œè¯·åˆ†æå…¶ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚æ•°æ®å›¾è¡¨è¶‹åŠ¿ã€å•†ä¸šæ¨¡å¼å›¾è§£ã€äº§å“åŸå‹ç‰¹å¾æˆ–è´¢åŠ¡é¢„æµ‹æ•°æ®ï¼‰ã€‚è¯·ç®€æ´æ˜äº†åœ°æè¿°å›¾ç‰‡å†…å®¹ã€‚"
            page_info = f"ç¬¬ {img['page']} é¡µ"
        
        response = client.chat.completions.create(
            model=config.VISION_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/{img['ext']};base64,{img['data']}"}
                        }
                    ],
                }
            ],
            max_tokens=500
        )
        description = response.choices[0].message.content
        logger.info(f"å›¾ç‰‡ {index + 1} åˆ†æå®Œæˆ")
        return (index, page_info, description)
        
    except Exception as e:
        logger.warning(f"åˆ†æå›¾ç‰‡ {index + 1} å¤±è´¥: {e}")
        return (index, None, None)


def describe_visual_elements(client, images: List[Dict]) -> str:
    """
    å¹¶å‘è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹å¯¹æå–çš„å›¾ç‰‡è¿›è¡Œç†è§£å’Œæè¿°ã€‚
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. ä½¿ç”¨ 3x3 æ‹¼å›¾ç­–ç•¥ï¼Œå°† 9 å¼ å›¾æ‹¼æˆ 1 å¼ ï¼ˆå‡å°‘ 89% API è¯·æ±‚ï¼‰
    2. ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘æ‰§è¡Œï¼ˆmax_workers=10ï¼‰
    """
    if not images:
        return "æœªå‘ç°æ˜¾è‘—è§†è§‰å…ƒç´ ã€‚"
    
    logger.info(f"æ£€æµ‹åˆ° {len(images)} å¼ æœ‰æ•ˆå›¾ç‰‡ï¼Œæ­£åœ¨è¿›è¡Œ 3x3 æ‹¼å›¾...")
    
    # 1. æ‹¼æ¥å›¾ç‰‡ï¼ˆ9 å¼ æ‹¼æˆ 1 å¼ ï¼‰
    stitched_images = stitch_images(images, grid_size=3)
    logger.info(f"å›¾ç‰‡æ‹¼æ¥å®Œæˆï¼š{len(images)} å¼  â†’ {len(stitched_images)} å¼ ï¼ˆå‡å°‘ {len(images) - len(stitched_images)} æ¬¡è¯·æ±‚ï¼‰")
    
    # 2. å¹¶å‘åˆ†ææ‹¼æ¥åçš„å›¾ç‰‡
    visual_context = "### ğŸ–¼ï¸ å•†ä¸šè®¡åˆ’ä¹¦è§†è§‰å…ƒç´ åˆ†æ\n"
    results = {}
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {
            executor.submit(_analyze_single_image, client, img, i): i 
            for i, img in enumerate(stitched_images)  # åˆ†ææ‹¼æ¥åçš„å›¾ç‰‡
        }
        
        for future in as_completed(futures):
            index, page_info, description = future.result()
            if description:
                results[index] = (page_info, description)
    
    # 3. æŒ‰åŸå§‹é¡ºåºç»„è£…ç»“æœ
    for i in sorted(results.keys()):
        page_info, description = results[i]
        visual_context += f"**[å›¾è¡¨ {i+1} ({page_info})]**: {description}\n\n"
    
    return visual_context


def google_search(query: str, start_id: int = 1) -> str:
    """
    ä½¿ç”¨ Serper.dev API è·å– Google æœç´¢ç»“æœã€‚
    """
    url = "https://google.serper.dev/search"
    headers = {
        'X-API-KEY': config.SERPER_API_KEY,
        'Content-Type': 'application/json'
    }
    payload = json.dumps({"q": query})

    try:
        logger.info(f"æ­£åœ¨å‘èµ·æœç´¢è¯·æ±‚: {query}")
        response = requests.post(url, headers=headers, data=payload, timeout=15)
        response.raise_for_status()
        search_data = response.json()
        
        results = []
        for i, item in enumerate(search_data.get('organic', [])[:5], start_id):
            title = item.get('title', 'æ— æ ‡é¢˜')
            snippet = item.get('snippet', 'æ— å†…å®¹')
            url_link = item.get('link', 'æ— é“¾æ¥')
            results.append(f"[S{i}] URL: {url_link} Title: {title} Snippet: {snippet}")
            
        if not results:
            logger.warning(f"å…³é”®è¯ '{query}' æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœã€‚")
            return ""
            
        return "\n".join(results)
    except requests.exceptions.RequestException as e:
        logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return f"ç½‘ç»œæœç´¢å¼‚å¸¸: {str(e)}"
    except Exception as e:
        logger.error(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
        return f"æœç´¢ç»“æœå¤„ç†å¼‚å¸¸: {str(e)}"


def clean_json_string(text: str) -> str:
    """
    ä» LLM è¾“å‡ºçš„åŸå§‹æ–‡æœ¬ä¸­æå– JSON å­—ç¬¦ä¸²ã€‚
    """
    json_block_pattern = r"```json\s*(.*?)\s*```"
    match = re.search(json_block_pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    braces_pattern = r"(\{.*\})"
    match = re.search(braces_pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    return text.strip()


def repair_json(json_str: str) -> str:
    """
    å°è¯•ä¿®å¤æˆªæ–­æˆ–æ ¼å¼é”™è¯¯çš„ JSON å­—ç¬¦ä¸²ã€‚
    
    ä¿®å¤ç­–ç•¥ï¼š
    1. å°è¯•ç›´æ¥è§£æ
    2. å¦‚æœå¤±è´¥ï¼Œå°è¯•è¡¥å…¨ç¼ºå¤±çš„å³å¤§æ‹¬å· } æˆ–å³ä¸­æ‹¬å· ]
    3. å¦‚æœä»ç„¶å¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸ {} å¹¶è®°å½•é”™è¯¯æ—¥å¿—
    
    å‚æ•°:
        json_str: å¾…ä¿®å¤çš„ JSON å­—ç¬¦ä¸²
    
    è¿”å›:
        ä¿®å¤åçš„ JSON å­—ç¬¦ä¸²ï¼ˆå¦‚æœæ— æ³•ä¿®å¤åˆ™è¿”å› "{}")
    """
    # ç¬¬ä¸€æ¬¡å°è¯•ï¼šç›´æ¥è§£æ
    try:
        json.loads(json_str)
        return json_str
    except json.JSONDecodeError as e:
        logger.warning(f"JSON è§£æå¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤: {e}")
    
    # ç¬¬äºŒæ¬¡å°è¯•ï¼šè¡¥å…¨ç¼ºå¤±çš„æ‹¬å·
    repaired = json_str.rstrip()
    
    # ç»Ÿè®¡æ‹¬å·æ•°é‡
    open_braces = repaired.count('{')
    close_braces = repaired.count('}')
    open_brackets = repaired.count('[')
    close_brackets = repaired.count(']')
    
    # è¡¥å…¨ç¼ºå¤±çš„å³ä¸­æ‹¬å·
    if open_brackets > close_brackets:
        repaired += ']' * (open_brackets - close_brackets)
        logger.info(f"è¡¥å…¨äº† {open_brackets - close_brackets} ä¸ªå³ä¸­æ‹¬å· ]")
    
    # è¡¥å…¨ç¼ºå¤±çš„å³å¤§æ‹¬å·
    if open_braces > close_braces:
        repaired += '}' * (open_braces - close_braces)
        logger.info(f"è¡¥å…¨äº† {open_braces - close_braces} ä¸ªå³å¤§æ‹¬å· }}")
    
    # å†æ¬¡å°è¯•è§£æ
    try:
        json.loads(repaired)
        logger.info("JSON ä¿®å¤æˆåŠŸ")
        return repaired
    except json.JSONDecodeError as e:
        logger.error(f"JSON ä¿®å¤å¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸: {e}")
        logger.error(f"åŸå§‹ JSON ç‰‡æ®µ: {json_str[:200]}...")
        return "{}"


def extract_funding_amounts(text: str) -> List[str]:
    """
    åˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ–‡æœ¬ä¸­æå–èèµ„é‡‘é¢ã€‚
    """
    patterns = [
        r"\d+\.?\d*\s*äº¿\s*(ç¾å…ƒ|å…ƒ|RMB|USD)?",
        r"\d+\.?\d*\s*ä¸‡\s*(ç¾å…ƒ|å…ƒ|RMB|USD)?",
        r"\d+\.?\d*\s*(million|billion)\s*(USD|RMB)?",
        r"[A-Z]è½®\d+\.?\d*äº¿"
    ]
    
    results = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            if isinstance(matches[0], tuple):
                full_matches = [m.group(0) for m in re.finditer(pattern, text, re.IGNORECASE)]
                results.extend(full_matches)
            else:
                results.extend(matches)
    
    return list(set(results))[:5]
