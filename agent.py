"""
agent.py
å•†ä¸šåˆ†ææ™ºèƒ½ä½“æ ¸å¿ƒæ¨¡å—ã€‚
è´Ÿè´£åè°ƒ PDF è§£æã€èµ›é“è¯†åˆ«ã€è”ç½‘æœç´¢åŠå…¨é¢†åŸŸ VC è§†è§’æ·±åº¦åˆ†æã€‚
ä¼˜åŒ–ç‰ˆï¼šæœç´¢æ¨¡å—å¹¶å‘æ‰§è¡Œï¼Œå¤§å¹…ç¼©çŸ­æ€»ä½“åˆ†ææ—¶é—´ã€‚
"""

import json
import logging
import traceback
from typing import List, Dict
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import config
import utils

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class BusinessResearcher:
    """
    å•†ä¸šç ”ç©¶å‘˜æ™ºèƒ½ä½“ï¼Œæ¨¡æ‹Ÿ VC åˆä¼™äººè¿›è¡Œé¡¹ç›®è¯„å®¡ã€‚
    """

    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“ã€‚
        
        å‚æ•°:
            api_key (str): LLM æˆæƒå¯†é’¥ã€‚
        """
        self.api_key = api_key
        self.client = OpenAI(
            api_key=api_key,
            base_url=config.LLM_BASE_URL
        )
        
        # è§†è§‰ä¸“ç”¨å®¢æˆ·ç«¯ (è§£å†³ 400 é”™è¯¯ï¼šè§£è€¦å¤šæ¨¡æ€ä¸æ–‡æœ¬è¯·æ±‚)
        vision_api_key = getattr(config, "VISION_API_KEY", api_key)
        vision_base_url = getattr(config, "VISION_BASE_URL", config.LLM_BASE_URL)
        
        # å¦‚æœè§†è§‰é…ç½®ä¸æ–‡æœ¬é…ç½®ä¸€è‡´ä¸”ä¸º DeepSeekï¼Œåˆ™ä¼šæœ‰å…¼å®¹æ€§é£é™©
        # å»ºè®®ç”¨æˆ·åœ¨ config.py ä¸­æ˜ç¡®é…ç½® VISION_BASE_URL ä¸ºæ”¯æŒ Vision çš„ç«¯ç‚¹
        self.vision_client = OpenAI(
            api_key=vision_api_key,
            base_url=vision_base_url,
            timeout=120.0  # å¢åŠ è¶…æ—¶é™åˆ¶ï¼Œé€‚åº”ä»£ç†ç¯å¢ƒä¸‹çš„å›¾ç‰‡ä¸Šä¼ 
        )

    def _detect_industry(self, bp_text: str) -> str:
        """
        ç¬¬ä¸€é˜¶æ®µï¼šè¯†åˆ«é¡¹ç›®ç»†åˆ†èµ›é“ã€‚
        
        å‚æ•°:
            bp_text (str): BP å…¨æ–‡ï¼ˆå¯åŒ…å«å›¾è¡¨æè¿°çš„å¢å¼ºæ–‡æœ¬ï¼‰ã€‚
            
        è¿”å›:
            str: è¯†åˆ«å‡ºçš„èµ›é“åç§°ã€‚
        """
        bp_snippet = bp_text[:20000]
        prompt = (
            "ä½ æ˜¯ä¸€åå…¨é¢†åŸŸ VC åˆä¼™äººï¼Œæ“…é•¿å¿«é€Ÿè¯†åˆ«åˆ›ä¸šé¡¹ç›®æ‰€å±çš„ç»†åˆ†èµ›é“ã€‚\n\n"
            "### ä»»åŠ¡\n"
            "é˜…è¯»ä»¥ä¸‹å•†ä¸šè®¡åˆ’ä¹¦æ‘˜è¦ï¼Œè¯†åˆ«è¯¥é¡¹ç›®å±äºå“ªä¸ª**ç»†åˆ†èµ›é“**ã€‚\n\n"
            "### è¾“å‡ºæ ¼å¼\n"
            "ç”¨ 'å¤§èµ›é“ - å°èµ›é“' çš„æ ¼å¼è¿”å›ï¼Œå¦‚ï¼š'æ™ºæ…§åŒ»ç–— - AI è¾…åŠ©è¯Šæ–­'ã€‚\n"
            "ä»…è¿”å›èµ›é“åç§°ï¼Œä¸è¦è¯´åºŸè¯ã€‚\n\n"
            f"å•†ä¸šè®¡åˆ’ä¹¦æ‘˜è¦ï¼š\n{bp_snippet}"
        )
        
        try:
            response = self.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            industry = response.choices[0].message.content.strip()
            logger.info(f"è¯†åˆ«åˆ°èµ›é“: {industry}")
            return industry
        except Exception as e:
            logger.error(f"èµ›é“è¯†åˆ«å¤±è´¥: {e}")
            return "å…¨é¢†åŸŸèµ›é“"

    def _get_search_keywords(self, bp_text: str, detected_industry: str) -> List[str]:
        """
        ç¬¬äºŒé˜¶æ®µï¼šåŸºäºèµ›é“ç”Ÿæˆä¸­è‹±æ–‡åŒè¯­æœç´¢å…³é”®è¯ã€‚
        
        å‚æ•°:
            bp_text (str): BP æ–‡æœ¬ï¼ˆå¯åŒ…å«å›¾è¡¨æè¿°çš„å¢å¼ºæ–‡æœ¬ï¼‰ã€‚
            detected_industry (str): è¯†åˆ«å‡ºçš„èµ›é“ã€‚
        """
        bp_snippet = bp_text[:30000]
        prompt = (
            "ä½ æ˜¯ä¸€åèµ„æ·±çš„å•†ä¸šæƒ…æŠ¥åˆ†æå¸ˆï¼Œæ“…é•¿ä»å•†ä¸šè®¡åˆ’ä¹¦ä¸­æå–é«˜è´¨é‡çš„ä¸­è‹±æ–‡åŒè¯­æœç´¢å…³é”®è¯ã€‚\n\n"
            f"é¡¹ç›®èµ›é“ï¼š**{detected_industry}**\n"
            "### ä»»åŠ¡\n"
            "æå– 10 ä¸ªç²¾å‡†æœç´¢å…³é”®è¯ï¼ˆ5 ä¸ªä¸­æ–‡ï¼Œ5 ä¸ªè‹±æ–‡ï¼‰ï¼Œç”¨äºåœ¨ Google æŸ¥æ‰¾å…¨çƒå¸‚åœºæ•°æ®ã€å›½é™…ç«å“å’Œå•†ä¸šæ¨¡å¼è¶‹åŠ¿ã€‚\n\n"
            "### ç­–ç•¥è¦æ±‚ï¼š\n"
            "1. **ä¸­æ–‡å…³é”®è¯**ï¼šè¦†ç›–å›½å†…å¸‚åœºè§„æ¨¡ã€æ”¿ç­–ç¯å¢ƒã€æœ¬åœŸç«å“åˆ†æã€‚\n"
            "2. **è‹±æ–‡å…³é”®è¯**ï¼šè¦†ç›–å…¨çƒè¡Œä¸šæŠ¥å‘Šï¼ˆGlobal Market Reportï¼‰ã€æµ·å¤–å·¨å¤´åŠ¨æ€ï¼ˆLeading Playersï¼‰ã€å›½é™…èèµ„è¶‹åŠ¿ï¼ˆFunding Trendsï¼‰ã€‚\n"
            "3. **å¿…é¡»åŒ…å«æ¨¡å¼å˜ä½“**ï¼šä¾‹å¦‚ \"{Industry} market size\"ã€\"{Competitor} revenue\"ã€\"{Industry} failure cases\"ã€‚\n\n"
            "### è¾“å‡ºæ ¼å¼\n"
            "ä»…è¿”å›å…³é”®è¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚ä¸è¦æœ‰ä»»ä½•è§£é‡Šã€‚\n\n"
            f"å•†ä¸šè®¡åˆ’ä¹¦ç‰‡æ®µï¼š\n{bp_snippet}"
        )
        
        try:
            response = self.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            keywords_text = response.choices[0].message.content.strip()
            keywords = [k.strip() for k in keywords_text.split(',')]
            return keywords[:10]
        except Exception as e:
            logger.error(f"æå–åŒè¯­å…³é”®è¯å¤±è´¥: {e}")
            return [
                f"{detected_industry} å¸‚åœºè§„æ¨¡", 
                f"{detected_industry} market size",
                f"{detected_industry} competitors analysis",
                f"{detected_industry} å•†ä¸šæ¨¡å¼",
                f"{detected_industry} business model",
                f"{detected_industry} global trends"
            ]

    def _concurrent_search(self, keywords: List[str]) -> Dict[str, str]:
        """
        å¹¶å‘æ‰§è¡Œ Google æœç´¢ï¼ˆæ€§èƒ½ä¼˜åŒ–å…³é”®ç‚¹ï¼‰ã€‚
        
        å‚æ•°:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            
        è¿”å›:
            Dict[å…³é”®è¯, æœç´¢ç»“æœ]
        """
        search_results = {}
        
        def search_worker(kw: str, idx: int):
            """å•ä¸ªæœç´¢ä»»åŠ¡"""
            start_id = idx * 5 + 1  # æ¯ä¸ªå…³é”®è¯æœç´¢ 5 æ¡ï¼ŒID ä¾æ¬¡ç´¯åŠ 
            result = utils.google_search(kw, start_id=start_id)
            return (kw, result)
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {
                executor.submit(search_worker, kw, i): kw 
                for i, kw in enumerate(keywords)
            }
            
            for future in as_completed(futures):
                kw, result = future.result()
                if result:
                    search_results[kw] = result
        
        logger.info(f"å¹¶å‘æœç´¢å®Œæˆï¼š{len(search_results)}/{len(keywords)} ä¸ªå…³é”®è¯è·å¾—ç»“æœ")
        return search_results

    def _generate_basic_info(self, fusion_context: str) -> Dict:
        """
        ã€å¹¶å‘å­ä»»åŠ¡ 1ã€‘ç”ŸæˆåŸºç¡€ä¿¡æ¯ç»„ï¼šproject_identity, industry_analysis, business_analysis
        
        åŸç†ï¼šè¿™ 3 ä¸ªå­—æ®µä¸»è¦æ¥è‡ª BP å†…å®¹æœ¬èº«ï¼Œæ— éœ€å¤æ‚æ¨ç†ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆã€‚
        
        å‚æ•°ï¼š
            fusion_context (str): èåˆåçš„ä¸Šä¸‹æ–‡ï¼ˆBP å†…å®¹ + æœç´¢ç»“æœï¼‰
        
        è¿”å›ï¼š
            Dict: åŒ…å« 3 ä¸ªå­—æ®µçš„ JSON å­—å…¸
        """
        try:
            response = self.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[
                    {"role": "system", "content": config.PROMPT_IDENTITY_BUSINESS},
                    {"role": "user", "content": fusion_context}
                ],
                temperature=0.3
            )
            raw_output = response.choices[0].message.content
            clean_json = utils.clean_json_string(raw_output)
            # ä½¿ç”¨ repair_json ä¿®å¤å¯èƒ½çš„æˆªæ–­é—®é¢˜
            repaired_json = utils.repair_json(clean_json)
            return json.loads(repaired_json)
        except Exception as e:
            logger.error(f"ç”ŸæˆåŸºç¡€ä¿¡æ¯ç»„å¤±è´¥: {e}")
            return {}

    def _generate_external_intel(self, fusion_context: str) -> Dict:
        """
        ã€å¹¶å‘å­ä»»åŠ¡ 2ã€‘ç”Ÿæˆå¤–éƒ¨æƒ…æŠ¥ç»„ï¼šcompetitors, funding_ecosystem, public_sentiment, raw_evidence
        
        åŸç†ï¼šè¿™ 4 ä¸ªå­—æ®µä¸»è¦åŸºäºè”ç½‘æœç´¢ç»“æœï¼Œå…³æ³¨å¤–éƒ¨å¸‚åœºæƒ…æŠ¥ã€‚
        
        å‚æ•°ï¼š
            fusion_context (str): èåˆåçš„ä¸Šä¸‹æ–‡ï¼ˆBP å†…å®¹ + æœç´¢ç»“æœï¼‰
        
        è¿”å›ï¼š
            Dict: åŒ…å« 4 ä¸ªå­—æ®µçš„ JSON å­—å…¸
        """
        try:
            response = self.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[
                    {"role": "system", "content": config.PROMPT_MARKET_COMPETITION},
                    {"role": "user", "content": fusion_context}
                ],
                temperature=0.3
            )
            raw_output = response.choices[0].message.content
            clean_json = utils.clean_json_string(raw_output)
            # ä½¿ç”¨ repair_json ä¿®å¤å¯èƒ½çš„æˆªæ–­é—®é¢˜
            repaired_json = utils.repair_json(clean_json)
            return json.loads(repaired_json)
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤–éƒ¨æƒ…æŠ¥ç»„å¤±è´¥: {e}")
            return {}

    def _generate_valuation(self, fusion_context: str) -> Dict:
        """
        ã€å¹¶å‘å­ä»»åŠ¡ 3ã€‘ç”Ÿæˆä¼°å€¼æ¨¡å‹ï¼švaluation_model
        
        åŸç†ï¼šä¸“æ³¨äºé‡åŒ–è¯„åˆ†ï¼Œå°†å¤æ‚çš„è¯„ä¼°ä»»åŠ¡æ‹†åˆ†ä¸ºç‹¬ç«‹çš„å¹¶å‘ä»»åŠ¡ã€‚
        
        å‚æ•°ï¼š
            fusion_context (str): èåˆåçš„ä¸Šä¸‹æ–‡ï¼ˆBP å†…å®¹ + æœç´¢ç»“æœï¼‰
        
        è¿”å›ï¼š
            Dict: åŒ…å« valuation_model å­—æ®µçš„ JSON å­—å…¸
        """
        try:
            response = self.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[
                    {"role": "system", "content": config.PROMPT_VALUATION},
                    {"role": "user", "content": fusion_context}
                ],
                temperature=0.3
            )
            raw_output = response.choices[0].message.content
            clean_json = utils.clean_json_string(raw_output)
            # ä½¿ç”¨ repair_json ä¿®å¤å¯èƒ½çš„æˆªæ–­é—®é¢˜
            repaired_json = utils.repair_json(clean_json)
            return json.loads(repaired_json)
        except Exception as e:
            logger.error(f"ç”Ÿæˆä¼°å€¼æ¨¡å‹å¤±è´¥: {e}")
            return {}

    def _generate_risks_and_qa(self, fusion_context: str) -> Dict:
        """
        ã€å¹¶å‘å­ä»»åŠ¡ 4ã€‘ç”Ÿæˆé£é™©è¯„ä¼°ä¸æ‹·é—®ï¼švc_grill, pain_point_validation, risk_assessment
        
        åŸç†ï¼šä¸“æ³¨äºé£é™©è¯†åˆ«å’Œå°–é”æé—®ï¼Œå‡è½»å•ä¸ªä»»åŠ¡çš„è´Ÿæ‹…ã€‚
        
        å‚æ•°ï¼š
            fusion_context (str): èåˆåçš„ä¸Šä¸‹æ–‡ï¼ˆBP å†…å®¹ + æœç´¢ç»“æœï¼‰
        
        è¿”å›ï¼š
            Dict: åŒ…å« 3 ä¸ªå­—æ®µçš„ JSON å­—å…¸
        """
        try:
            response = self.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[
                    {"role": "system", "content": config.PROMPT_RISK_QA},
                    {"role": "user", "content": fusion_context}
                ],
                temperature=0.3
            )
            raw_output = response.choices[0].message.content
            clean_json = utils.clean_json_string(raw_output)
            # ä½¿ç”¨ repair_json ä¿®å¤å¯èƒ½çš„æˆªæ–­é—®é¢˜
            repaired_json = utils.repair_json(clean_json)
            return json.loads(repaired_json)
        except Exception as e:
            logger.error(f"ç”Ÿæˆé£é™©è¯„ä¼°ç»„å¤±è´¥: {e}")
            return {}

    def analyze_bp_pipeline(self, pdf_path: str) -> Dict:
        """
        å…¨æµç¨‹å•†ä¸šåˆ†ææµæ°´çº¿ï¼ˆå‡çº§ç‰ˆï¼šè§†è§‰ä¿¡æ¯å‰ç½®èåˆï¼Œç¡®ä¿å›¾ç‰‡å‹ PDF åˆ†ææœ‰æ•ˆæ€§ï¼‰ã€‚
        
        å‚æ•°:
            pdf_path (str): PDF æ–‡ä»¶è·¯å¾„ã€‚
            
        è¿”å›:
            Dict: æ ¼å¼åŒ–çš„å•†ä¸šåˆ†æ JSON æŠ¥å‘Šã€‚
        """
        try:
            # 1. æ–‡æœ¬ä¸å›¾åƒæå–
            logger.info(f"å¼€å¯æµæ°´çº¿åˆ†æï¼ˆå¤šæ¨¡æ€+å¹¶å‘ä¼˜åŒ–ï¼‰ï¼Œå¤„ç†æ–‡ä»¶: {pdf_path}")
            pdf_content = utils.extract_content_from_pdf(pdf_path)
            bp_full_text = pdf_content["text"]
            bp_images = pdf_content["images"]
            
            if "å¤±è´¥" in bp_full_text:
                return {"error": "PDF å†…å®¹æ— æ³•è¯»å–"}

            # 2. è§†è§‰å†…å®¹è§£æï¼ˆå‰ç½®åˆ°èµ›é“è¯†åˆ«ä¹‹å‰ï¼Œå…³é”®ä¿®å¤ç‚¹ï¼‰
            visual_descriptions = ""
            if bp_images:
                logger.info(f"æ£€æµ‹åˆ° {len(bp_images)} å¼ æœ‰æ•ˆå›¾ç‰‡ï¼Œæ­£åœ¨å‘èµ·å¹¶å‘è§†è§‰åˆ†æ...")
                visual_descriptions = utils.describe_visual_elements(self.vision_client, bp_images)
                logger.info(f"è§†è§‰åˆ†æå®Œæˆï¼Œæå–äº† {len(visual_descriptions)} å­—ç¬¦çš„å›¾è¡¨æè¿°")

            # 2.5 åˆ›å»ºå¢å¼ºæ–‡æœ¬ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šèåˆåŸæ–‡æœ¬ä¸è§†è§‰æè¿°ï¼‰
            enhanced_text = bp_full_text
            if visual_descriptions and visual_descriptions != "æœªå‘ç°æ˜¾è‘—è§†è§‰å…ƒç´ ã€‚":
                logger.info("æ­£åœ¨èåˆæ–‡æœ¬ä¸è§†è§‰ä¿¡æ¯ï¼Œç”Ÿæˆå¢å¼ºåˆ†æä¸Šä¸‹æ–‡...")
                enhanced_text = f"{bp_full_text}\n\n{visual_descriptions}"
            else:
                logger.warning("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè§†è§‰å†…å®¹ï¼Œä»…ä½¿ç”¨çº¯æ–‡æœ¬è¿›è¡Œåˆ†æ")

            # 3. èµ›é“æ„ŸçŸ¥ï¼ˆç°åœ¨åŸºäºå¢å¼ºæ–‡æœ¬ï¼Œå›¾ç‰‡å‹ PDF ä¹Ÿèƒ½å‡†ç¡®è¯†åˆ«ï¼‰
            detected_industry = self._detect_industry(enhanced_text)
            
            # 4. å…³é”®è¯è·å–ï¼ˆç°åœ¨åŸºäºå¢å¼ºæ–‡æœ¬ï¼Œå…³é”®è¯æ›´ç²¾å‡†ï¼‰
            keywords = self._get_search_keywords(enhanced_text, detected_industry)
            
            # 5. å¹¶å‘è”ç½‘æ£€ç´¢ï¼ˆæ€§èƒ½ä¼˜åŒ–å…³é”®ç‚¹ï¼‰
            logger.info("æ­£åœ¨å¹¶å‘æ‰§è¡Œ Google æœç´¢...")
            search_results = self._concurrent_search(keywords)
            
            # ç»„è£…æœç´¢ä¸Šä¸‹æ–‡
            search_context = ""
            for kw, result in search_results.items():
                search_context += f"--- å…³é”®è¯: {kw} ---\n{result}\n"

            # 6. å¹¶å‘ JSON ç”Ÿæˆï¼ˆæ€§èƒ½ä¼˜åŒ–å…³é”®ç‚¹ï¼šå°†é•¿æ–‡æœ¬ç”Ÿæˆæ‹†åˆ†ä¸º 4 ä¸ªå­ä»»åŠ¡ï¼‰
            logger.info("å‘èµ·å¹¶å‘ JSON ç”Ÿæˆï¼ˆ4 ä¸ªå­ä»»åŠ¡ï¼‰...")
            bp_summary = enhanced_text[:30000]  # ä½¿ç”¨å¢å¼ºæ–‡æœ¬è€ŒéåŸå§‹æ–‡æœ¬
            fusion_context = (
                f"### ğŸ“„ å•†ä¸šè®¡åˆ’ä¹¦å†…å®¹æ‘˜è¦ï¼ˆåŒ…å«æ–‡æœ¬ä¸å›¾è¡¨è§£æï¼‰\n{bp_summary}\n\n"
                f"### ğŸ” å¤–éƒ¨æœç´¢æƒ…æŠ¥\n{search_context}"
            )
            
            # å¹¶å‘è°ƒç”¨ 4 ä¸ªç”Ÿæˆæ–¹æ³•ï¼ˆMap-Reduce æ¨¡å¼ï¼‰
            result = {}
            with ThreadPoolExecutor(max_workers=4) as executor:
                # æäº¤ 4 ä¸ªå¹¶å‘ä»»åŠ¡
                future_basic = executor.submit(self._generate_basic_info, fusion_context)
                future_intel = executor.submit(self._generate_external_intel, fusion_context)
                future_valuation = executor.submit(self._generate_valuation, fusion_context)
                future_risks = executor.submit(self._generate_risks_and_qa, fusion_context)
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶åˆå¹¶ç»“æœ
                basic_info = future_basic.result()
                external_intel = future_intel.result()
                valuation_data = future_valuation.result()
                risks_qa_data = future_risks.result()
                
                # åˆå¹¶ 4 ä¸ªå­—å…¸
                result.update(basic_info)
                result.update(external_intel)
                result.update(valuation_data)
                result.update(risks_qa_data)
            
            logger.info("å¹¶å‘ JSON ç”Ÿæˆå®Œæˆï¼Œæ­£åœ¨æ ¡éªŒå®Œæ•´æ€§...")

            # 7. JSON å®Œæ•´æ€§æ ¡éªŒä¸å…œåº•ï¼ˆæ–°å¢ valuation_modelï¼‰
            required_keys = ["project_identity", "industry_analysis", "business_analysis", "competitors", "raw_evidence", "vc_grill", "valuation_model", "funding_ecosystem", "pain_point_validation", "public_sentiment", "risk_assessment"]
            for key in required_keys:
                if key not in result:
                    logger.warning(f"å­—æ®µ {key} ç¼ºå¤±ï¼Œæ­£åœ¨è¿›è¡Œé»˜è®¤å¡«å……ã€‚")
                    if key == "project_identity":
                        result[key] = {
                            "project_name": "Unknown Project",
                            "slogan": "N/A",
                            "description": "æœªèƒ½ä» BP ä¸­æå–æ·±åº¦æè¿°",
                            "revenue_model": "N/A",
                            "team_background": "Not Mentioned",
                            "stage": "æœªçŸ¥"
                        }
                    elif key == "industry_analysis":
                        result[key] = {"detected_industry": detected_industry, "market_size": "Not Found", "cagr": "Not Found", "source": "N/A"}
                    elif key == "business_analysis":
                        result[key] = {"business_model_critique": "N/A", "technical_moat": "N/A"}
                    elif key == "competitors": result[key] = []
                    elif key == "raw_evidence": result[key] = []
                    elif key == "vc_grill": result[key] = []
                    elif key == "valuation_model":
                        result[key] = {
                            "total_score": 50,
                            "rating": "C",
                            "summary": "æ•°æ®ä¸¥é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå…¨é¢é‡åŒ–è¯„ä¼°",
                            "dimensions": {
                                "market": {
                                    "score": 10,
                                    "max_score": 20,
                                    "analysis": "ç¼ºä¹å¸‚åœºè§„æ¨¡ä¸å¢é•¿æ•°æ®",
                                    "sub_scores": {"market_size": 5, "timing_growth": 5}
                                },
                                "product": {
                                    "score": 10,
                                    "max_score": 25,
                                    "analysis": "æœªèƒ½è¯†åˆ«æ ¸å¿ƒæŠ€æœ¯å£å’ä¸åˆ›æ–°ç‚¹",
                                    "sub_scores": {"uniqueness": 5, "moat": 5}
                                },
                                "business_model": {
                                    "score": 10,
                                    "max_score": 20,
                                    "analysis": "å•†ä¸šæ¨¡å¼ä¸ç›ˆåˆ©è·¯å¾„ä¸æ˜ç¡®",
                                    "sub_scores": {"profitability": 5, "scalability": 5}
                                },
                                "team": {
                                    "score": 10,
                                    "max_score": 25,
                                    "analysis": "BP ä¸­æœªæåŠæ ¸å¿ƒå›¢é˜ŸèƒŒæ™¯",
                                    "sub_scores": {"founder_capability": 5, "completeness": 5}
                                },
                                "execution": {
                                    "score": 10,
                                    "max_score": 10,
                                    "analysis": "ç¼ºä¹ä¸šåŠ¡éªŒè¯ä¸åˆè§„é£é™©è¯„ä¼°ä¾æ®",
                                    "sub_scores": {"traction": 5, "risk_safety": 5}
                                }
                            }
                        }
                    elif key == "funding_ecosystem": result[key] = {"heat_level": "Unknown", "trend_summary": "Not Found"}
                    elif key == "pain_point_validation": result[key] = {"score": 0, "reason": "N/A"}
                    elif key == "public_sentiment": result[key] = {"label": "Neutral", "summary": "Not Found"}
                    elif key == "risk_assessment": result[key] = ["è¯†åˆ«é£é™©å¤±è´¥"]

            logger.info("åˆ†ææµç¨‹åœ†æ»¡å®Œæˆã€‚")
            return result

        except Exception as e:
            logger.error(f"åˆ†ææµæ°´çº¿å´©æºƒ: {e}\n{traceback.format_exc()}")
            return {
                "error": "Pipeline Failure",
                "details": str(e),
                "template": {
                    "industry_analysis": {"detected_industry": "Error", "market_size": "Not Found", "cagr": "Not Found", "source": "N/A"},
                    "competitors": [],
                    "funding_ecosystem": {"heat_level": "Unknown", "trend_summary": "N/A"},
                    "pain_point_validation": {"score": 0, "reason": "N/A"},
                    "public_sentiment": {"label": "Neutral", "summary": "N/A"},
                    "risk_assessment": ["å†…éƒ¨ç³»ç»Ÿå¼‚å¸¸"]
                }
            }
