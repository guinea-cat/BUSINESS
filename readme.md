这个系统是一个典型的**多模态 RAG（检索增强生成）智能体**工作流，模拟 VC（风险投资人）阅读商业计划书（BP）、查阅资料并撰写报告的过程。

以下是代码实现的详细工作流拆解，以及针对 PDF 处理的具体限制分析和优化建议。

---

### 1. 核心工作流程 (Workflow)

整个流程由用户在 Gradio 界面上传 PDF 开始，经历 **“解析 -> 感知 -> 搜索 -> 融合 -> 生成”** 五个阶段：

#### **第一阶段：文件解析 (PDF Extraction)**
*   **输入**：用户上传的 PDF 文件。
*   **动作**：
    1.  使用 `PyMuPDF (fitz)` 库读取文件。
    2.  **提取文字**：遍历每一页，将所有文本拼接成一个长字符串。
    3.  **提取图片**：遍历每一页提取嵌入的图片对象。
*   **输出**：纯文本字符串 + 图片列表（Base64编码）。

#### **第二阶段：视觉与赛道感知 (Perception)**
*   **视觉分析 (Vision)**：
    *   将提取出的图片发送给 **Qwen-VL-Plus (通义千问多模态模型)**。
    *   让模型描述图片内容（重点关注图表、架构图、财务数据）。
    *   生成一段《视觉分析摘要》。
*   **赛道识别 (Text)**：
    *   截取 PDF 文本的前 **3000** 字符。
    *   发送给 **DeepSeek** 模型，快速判断项目属于哪个“大赛道-小赛道”。

#### **第三阶段：情报检索 (Search)**
*   **关键词生成**：
    *   基于 PDF 文本（前 20000 字符）和识别出的赛道，让 LLM 生成 10 个中英文搜索关键词（例如：“智慧医疗 市场规模”、“AI Diagnosis market size”）。
*   **联网搜索**：
    *   利用 **Serper.dev API** (Google Search) 逐个搜索关键词。
    *   提取搜索结果的 标题、摘要、URL。

#### **第四阶段：深度融合分析 (Fusion & Analysis)**
*   **上下文组装**：将以下三部分拼装成一个巨大的 Prompt：
    1.  **文本摘要**（PDF 文本的前 5000 字符）。
    2.  **视觉描述**（来自第二阶段的图片分析文字）。
    3.  **搜索情报**（来自第三阶段的 Google 搜索结果）。
*   **LLM 推理**：
    *   调用 **DeepSeek** 模型。
    *   使用 `SYSTEM_PROMPT` 扮演 VC 合伙人角色。
    *   要求模型根据上述信息，输出严格的 JSON 格式报告。

#### **第五阶段：报告渲染 (Rendering)**
*   **格式化**：解析 JSON，进行完整性校验（缺字段则补默认值）。
*   **展示**：将 JSON 转换为 Markdown 格式，在前端渲染出包含“项目画像、赛道分析、VC 灵魂拷问”的研报。

---

### 2. PDF 处理细节与限制 (PDF Processing Limits)

在 `utils.py` 和 `agent.py` 中，对信息量做了明确的**硬性限制**，这是为了节省 Token 成本和防止 API 超时：

#### **A. 文字处理 (Text Limits)**
代码中有三处截断（Truncation）：
1.  **赛道识别时**：只读取前 **3,000** 个字符 (`agent.py: line 55`)。
    *   *影响*：如果 BP 前几页全是废话，赛道可能识别不准。
2.  **生成关键词时**：只读取前 **20,000** 个字符 (`agent.py: line 88`)。
    *   *影响*：覆盖面较广，基本够用。
3.  **最终深度分析时**：只读取前 **5,000** 个字符 (`agent.py: line 150`)。
    *   *严重影响*：**这是最大的瓶颈**。如果 BP 很长（比如 50 页），第 10 页之后的内容（通常包含详细财务预测、风险披露、具体运营数据）会被直接丢弃，LLM 根本看不到。

#### **B. 图片处理 (Image Limits)**
1.  **数量限制**：最多只处理 **前 5 张** 图片 (`utils.py: line 79`)。
    *   *逻辑*：`images_base64[:5]`。
    *   *风险*：BP 的前几张图通常是封面背景、Logo 或团队照片。真正有价值的“市场趋势图”、“技术架构图”或“财务表”往往在中间或后面，可能被遗漏。
2.  **体积/尺寸限制**：
    *   **过滤**：小于 5KB 的图片（通常是图标、装饰线）直接丢弃。
    *   **压缩**：如果图片宽度 > 1024px，会强制缩小到 1024px 宽。
    *   **格式**：强制转为 JPEG 且压缩质量为 75。

---

### 3. 代码优化建议 (Optimization Strategy)

针对上述流程和限制，我有以下 5 点具体的优化建议：

#### **1. 解决“烂尾”问题（突破 5000 字符限制）**
DeepSeek 和 Qwen 目前都支持长上下文（Context Window 通常在 32k - 128k）。
*   **建议**：去掉 `bp_full_text[:5000]` 的切片操作，或者将其扩大到 `[:50000]`。现在的 Token 价格很便宜，没必要为了省钱而丢弃 80% 的 BP 内容，导致分析结果出现幻觉（因为 LLM 看不到后半部分的财务数据，只能瞎编或填 "Not Found"）。

#### **2. 优化图片筛选策略（Smart Image Selection）**
当前的逻辑是“取前 5 张”，这很笨拙。
*   **建议**：
    *   **跳过前 2 张**：通常是封面和目录，含金量低。
    *   **基于长宽比筛选**：优先保留横向的长图（通常是图表或时间轴），剔除正方形的小图（通常是头像）。
    *   **OCR 预判**：先用轻量级 OCR 扫一眼，如果图中包含 "RMB", "Revenue", "%", "2025E" 等关键词，则判定为高价值图片，优先保留。

#### **3. 引入“思维链”搜索（CoT Search）**
当前是“生成关键词 -> 搜索 -> 总结”，这是单向的。
*   **建议**：改为 Agent 模式。让 LLM 先看 BP，指出缺什么数据（比如：“我不知道这个行业的 CAGR 是多少”），然后针对性地去搜索，而不是盲目生成 10 个关键词。

#### **4. 增强 PDF 解析库**
`PyMuPDF` 对扫描版 PDF（纯图片型 PDF）束手无策，提取出来的文字是空的。
*   **建议**：集成 `OCR`（如 PaddleOCR 或 Tesseract）。如果 `full_text` 长度小于 100 字，自动触发 OCR 流程，把图片转为文字。

#### **5. 结构化输出的稳定性**
目前的 Prompt 依赖 `clean_json_string` 正则提取，LLM 偶尔还是会输出不合法的 JSON（比如未转义的引号）。
*   **建议**：
    *   使用 DeepSeek 的 **JSON Mode**（如果 API 支持）。
    *   或者使用 `Pydantic` 配合 `Instructor` 库，强制 LLM 输出符合 Python 类定义的结构化数据，彻底解决解析报错问题。

#### **修改代码示例 (扩大文本上下文)**

在 `agent.py` 的 `analyze_bp_pipeline` 方法中：

```python
# 修改前
bp_summary = bp_full_text[:5000] 

# 修改后：根据模型能力调整，DeepSeek-V3 支持 64k+，建议至少给 30k
# 甚至可以做一个简单的 token 计算
bp_summary = bp_full_text[:30000] 
if len(bp_summary) < len(bp_full_text):
    logger.warning("BP 内容过长，已截断处理")
```