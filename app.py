"""
app.py
Gradio Web ç•Œé¢æ¼”ç¤ºã€‚
"""

import os
import gradio as gr
import config
import logging
import time
from datetime import datetime
from agent import BusinessResearcher

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_analysis(file_obj):
    """
    å¤„ç†ä¸Šä¼ æ–‡ä»¶å¹¶è°ƒç”¨åˆ†ææµæ°´çº¿ã€‚
    ä½¿ç”¨ yield æœºåˆ¶å®æ—¶è¿”å›è¿›åº¦çŠ¶æ€ï¼ˆä¼˜åŒ–ç”¨æˆ·ä½“éªŒï¼‰ã€‚
    """
    if file_obj is None:
        yield "# âš ï¸ è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶", {}, None
        return
    
    try:
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        pdf_path = file_obj.name
        researcher = BusinessResearcher(config.LLM_API_KEY)
        
        # é˜¶æ®µ 1ï¼šPDF è§£æ
        elapsed = time.time() - start_time
        yield f"## ğŸ“„ æ­£åœ¨è§£æ PDF ä¸å›¾ç‰‡... (å·²è€—æ—¶ {elapsed:.1f}s)\n\nè¯·ç¨å€™ï¼Œç³»ç»Ÿæ­£åœ¨æå–æ–‡æœ¬å†…å®¹å’Œè§†è§‰å…ƒç´ ã€‚", {}, None
        
        import utils
        pdf_content = utils.extract_content_from_pdf(pdf_path)
        bp_full_text = pdf_content["text"]
        bp_images = pdf_content["images"]
        
        # é˜¶æ®µ 2ï¼šè§†è§‰åˆ†æ
        if bp_images:
            elapsed = time.time() - start_time
            yield f"## ğŸ–¼ï¸ æ­£åœ¨å¹¶å‘è§†è§‰åˆ†æ... (å·²è€—æ—¶ {elapsed:.1f}s)\n\næ£€æµ‹åˆ° {len(bp_images)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨è¿›è¡Œ 2x2 æ‹¼å›¾ä¸å¹¶å‘åˆ†æã€‚", {}, None
            visual_descriptions = utils.describe_visual_elements(researcher.vision_client, bp_images)
        else:
            visual_descriptions = ""
        
        # é˜¶æ®µ 3-4ï¼šèµ›é“æ„ŸçŸ¥ä¸å…³é”®è¯ç”Ÿæˆï¼ˆåˆå¹¶ä¼˜åŒ–ï¼‰
        elapsed = time.time() - start_time
        yield f"## ğŸ¯ æ­£åœ¨è¿›è¡Œèµ›é“æ„ŸçŸ¥ä¸å…³é”®è¯ç”Ÿæˆ... (å·²è€—æ—¶ {elapsed:.1f}s)\n\nã€æ€§èƒ½ä¼˜åŒ–ã€‘å•æ¬¡ LLM è°ƒç”¨åŒæ—¶å®Œæˆèµ›é“è¯†åˆ«å’Œå…³é”®è¯ç”Ÿæˆï¼ŒèŠ‚çœ 2-3 ç§’ã€‚", {}, None
        
        # é˜¶æ®µ 5ï¼šå¹¶å‘ JSON ç”Ÿæˆ
        elapsed = time.time() - start_time
        yield f"## ğŸ§  æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç ”æŠ¥... (å·²è€—æ—¶ {elapsed:.1f}s)\n\nå¹¶å‘æ‰§è¡Œ **4 è·¯å¹¶å‘åˆ†æ**ï¼š\n- åŸºç¡€ä¿¡æ¯ç»„ï¼ˆé¡¹ç›®ç”»åƒ + èµ›é“åˆ†æï¼‰\n- å¤–éƒ¨æƒ…æŠ¥ç»„ï¼ˆç«å“ + èèµ„ç”Ÿæ€ï¼‰\n- **ä¼°å€¼æ¨¡å‹ç»„**ï¼ˆVC è¯„åˆ†ï¼‰\n- **é£é™©è¯„ä¼°ç»„**ï¼ˆæ‹·é—® + ç—›ç‚¹ + é£é™©ï¼‰", {}, None
        
        # è°ƒç”¨å®Œæ•´çš„åˆ†ææµæ°´çº¿
        result = researcher.analyze_bp_pipeline(pdf_path)
        
        # è®¡ç®—æ€»è€—æ—¶
        total_time = time.time() - start_time
        
        # æ ¼å¼åŒ– Markdown æŠ¥å‘Šï¼ˆåŒ…å«æ€»è€—æ—¶ï¼‰
        markdown_report = format_markdown(result, total_time)
        
        # --- æ–°å¢ï¼šä¿å­˜å¹¶å¯¼å‡º MD æ–‡ä»¶ ---
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "reports"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        # æ„é€ æ–‡ä»¶åï¼šé¡¹ç›®å_ç ”æŠ¥_æ—¶é—´æˆ³.md
        project_name = result.get("project_identity", {}).get("project_name", "æœªçŸ¥é¡¹ç›®")
        # æ¸…æ´—æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        safe_project_name = "".join([c for c in project_name if c.isalnum() or c in (" ", "_", "-")]).strip()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"{safe_project_name}_ç ”æŠ¥_{timestamp}.md"
        file_path = os.path.abspath(os.path.join(output_dir, file_name))
        
        # å†™å…¥æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(markdown_report)
            
        logger.info(f"ç ”æŠ¥å·²æˆåŠŸä¿å­˜è‡³: {file_path}")
        # -----------------------------
        
        yield markdown_report, result, file_path
        
    except Exception as e:
        logger.error(f"UI å¤„ç†å¼‚å¸¸: {e}")
        yield f"# âŒ ç³»ç»Ÿå¤„ç†å¼‚å¸¸\n{str(e)}", {"status": "error"}, None

def format_markdown(data: dict, total_time: float = 0) -> str:
    """
    å°†åˆ†æç»“æœ JSON è½¬æ¢ä¸º Markdown ç ”æŠ¥ã€‚
    
    å‚æ•°:
        data: åˆ†æç»“æœå­—å…¸
        total_time: æ€»è€—æ—¶ï¼ˆç§’ï¼‰
    """
    from datetime import datetime
    import re
    
    if "error" in data:
        return f"# âš ï¸ é”™è¯¯\n\n{data.get('error')}\n\n**è¯¦ç»†ä¿¡æ¯**: {data.get('details', 'N/A')}"
    
    # è¾…åŠ©å‡½æ•°ï¼šå°† [S1] è½¬æ¢ä¸ºä¸Šæ ‡æ ¼å¼
    def cite_repl(match):
        return f"<sup>{match.group(0)}</sup>"
    
    def process_citations(text: str) -> str:
        if not isinstance(text, str): return str(text)
        return re.sub(r"\[S\d+\]", cite_repl, text)

    md = "# ğŸ“Š ç§‘åˆ›å¤§èµ› AI è¯„å®¡ - æ·±åº¦å•†ä¸šåˆ†ææŠ¥å‘Š\n\n---\n\n"
    
    # 1. é¡¹ç›®æ·±åº¦ç”»åƒ
    pi = data.get("project_identity", {})
    md += f"## ğŸš€ é¡¹ç›®æœ¬ä½“ç”»åƒ (Project Identity)\n"
    md += f"**é¡¹ç›®åç§°**: {pi.get('project_name', 'N/A')}\n\n"
    md += f"**æ ¸å¿ƒæ„¿æ™¯**: *{pi.get('slogan', 'N/A')}*\n\n"
    md += f"### ğŸ“ æ·±åº¦æè¿°\n{process_citations(pi.get('description', 'N/A'))}\n\n"
    md += f"### ğŸ’° ç›ˆåˆ©æ¨¡å¼\n{process_citations(pi.get('revenue_model', 'N/A'))}\n\n"
    # md += f"### ğŸ‘¥ å›¢é˜ŸèƒŒæ™¯ä¼˜åŠ¿\n{process_citations(pi.get('team_background', 'N/A'))}\n\n"
    md += f"- **å‘å±•é˜¶æ®µ**: `{pi.get('stage', 'N/A')}`\n\n"
    
    # 2. èµ›é“ä¸å¸‚åœº
    ia = data.get("industry_analysis", {})
    md += "## ğŸŒ èµ›é“åˆ†æä¸å¸‚åœºé‡åŒ–\n"
    md += f"- **è¯†åˆ«èµ›é“**: {ia.get('detected_industry', 'N/A')}\n"
    # md += f"- **å¸‚åœºè§„æ¨¡**: {process_citations(ia.get('market_size', 'Not Found'))}\n"
    # md += f"- **å¤åˆå¢é•¿ç‡ (CAGR)**: {process_citations(ia.get('cagr', 'Not Found'))}\n"
    # md += f"- **æ•°æ®æ¥æº**: {ia.get('source', 'N/A')}\n\n"

    # 3. å•†ä¸šæ·±åº¦æ‹†è§£
    ba = data.get("business_analysis", {})
    md += "## âš–ï¸ å•†ä¸šæ·±åº¦æ‹†è§£\n"
    md += f"### ğŸ¢ å•†ä¸šæ¨¡å¼å¯è¡Œæ€§è¯„è¿°\n{process_citations(ba.get('business_model_critique', 'N/A'))}\n\n"
    md += f"### ğŸ›¡ï¸ æŠ€æœ¯å£å’ä¸æŠ¤åŸæ²³\n{process_citations(ba.get('technical_moat', 'N/A'))}\n\n"
    
    # 4. å•†ä¸šæ½œåŠ›é‡åŒ–è¯„ä¼° (æ–°å¢)
    vm = data.get("valuation_model", {})
    if vm:
        md += f"## ğŸ’ å•†ä¸šæ½œåŠ›é‡åŒ–è¯„ä¼° (Valuation Model)\n"
        md += f"**ç»¼åˆè¯„åˆ†**: `{vm.get('total_score', 'N/A')}` | **æŠ•èµ„è¯„çº§**: `{vm.get('rating', 'N/A')}`\n\n"
        md += f"> **æ ¸å¿ƒæ‘˜è¦**: {process_citations(vm.get('summary', 'N/A'))}\n\n"
        
        md += "| è¯„ä¼°ç»´åº¦ | åˆ†æ•° | æ»¡åˆ† | æ ¸å¿ƒåˆ†æ |\n"
        md += "| :--- | :--- | :--- | :--- |\n"
        
        dimensions = vm.get("dimensions", {})
        dim_map = {
            "market": "å¸‚åœºæ½œåŠ›",
            "product": "äº§å“ä¸æŠ€æœ¯",
            "business_model": "å•†ä¸šæ¨¡å¼",
            "team": "å›¢é˜Ÿç«äº‰åŠ›",
            "execution": "éªŒè¯ä¸é£é™©"
        }
        
        for key, label in dim_map.items():
            d = dimensions.get(key, {})
            score = d.get("score", "N/A")
            max_s = d.get("max_score", "N/A")
            analysis = process_citations(d.get("analysis", "N/A"))
            md += f"| {label} | {score} | {max_s} | {analysis} |\n"
        md += "\n"

    # 5. VC çµé­‚æ‹·é—®
    vg = data.get("vc_grill", [])
    if vg:
        md += "## ğŸ”¥ VC çµé­‚æ‹·é—® (The VC Grill)\n"
        for item in vg:
            md += f"**Q: {item.get('question')}**\n\n"
            md += f"**A:** {process_citations(item.get('answer'))}\n\n"
    
    # 6. ç—›ç‚¹çœŸå®æ€§éªŒè¯ (æ–°å¢)
    ppv = data.get("pain_point_validation", {})
    if ppv:
        md += "## ğŸ¯ ç—›ç‚¹çœŸå®æ€§éªŒè¯\n"
        md += f"**çœŸå®æ€§è¯„åˆ†**: `{ppv.get('score', 'N/A')}/10`\n\n"
        md += f"**è¯„ä¼°é€»è¾‘**: {process_citations(ppv.get('reason', 'N/A'))}\n\n"

    # 7. ç«å“
    md += "## ğŸ¯ ç«äº‰æ ¼å±€ä¸æ›¿ä»£å“\n"
    for comp in data.get("competitors", []):
        md += f"### ğŸ¢ {comp.get('name')}\n- **ç±»å‹**: {comp.get('type')}\n- **åˆ†æ**: {process_citations(comp.get('comparison'))}\n\n"
    
    # 8. èèµ„ä¸èˆ†æƒ…
    fe = data.get("funding_ecosystem", {})
    ps = data.get("public_sentiment", {})
    md += f"## ğŸ’¹ èèµ„ç”Ÿæ€ & èˆ†æƒ…ç ”åˆ¤\n"
    md += f"- **èµ„æœ¬çƒ­åº¦**: `{fe.get('heat_level', 'N/A')}`\n"
    md += f"- **åŠ¨æ€æ‘˜è¦**: {process_citations(fe.get('trend_summary', 'N/A'))}\n"
    md += f"- **èˆ†æƒ…å€¾å‘**: {ps.get('label')} â€” {process_citations(ps.get('summary'))}\n\n"
    
    # 9. é£é™©
    md += "## âš ï¸ æ ¸å¿ƒé£é™©è¯†åˆ«\n"
    for risk in data.get("risk_assessment", []):
        md += f"- {process_citations(risk)}\n"
    
    # 10. æ•°æ®æ¥æºä¸å‚è€ƒæ–‡çŒ®
    md += "\n---\n## ğŸ”— æ•°æ®æ¥æºä¸å‚è€ƒæ–‡çŒ®\n"
    evidence = data.get("raw_evidence", [])
    if evidence:
        for item in evidence:
            eid = item.get('id', 'N/A')
            md += f"- **[{eid}] {item.get('source')}**: [{item.get('url')}]({item.get('url')})\n"
    else:
        md += "- æš‚æ— å¤–éƒ¨å‚è€ƒé“¾æ¥ã€‚\n"
    
    md += f"\n---\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n"
    md += f"*æœ¬æ¬¡åˆ†ææ€»è€—æ—¶: {total_time:.1f} ç§’*"
    return md

def main():
    with gr.Blocks(title="SAGE Business Analysis", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸš€ SAGE å•†ä¸šæ½œåŠ› AI è¯„æµ‹ç³»ç»Ÿ\nåŸºäº DeepSeek & Serper.dev çš„å…¨è¡Œä¸šé€šç”¨åˆ†æå¼•æ“ã€‚")
        
        with gr.Row():
            with gr.Column(scale=1):
                pdf_input = gr.File(label="ä¸Šä¼  BP (PDF)", file_types=[".pdf"])
                btn = gr.Button("å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ", variant="primary")
                out_file = gr.File(label="ğŸ“¥ ä¸‹è½½ç ”æŠ¥ (.md)", interactive=False)
                gr.Markdown("### âš™ï¸ è¯´æ˜\n- ç³»ç»Ÿå°†è‡ªåŠ¨è¯†åˆ«èµ›é“å¹¶è¿›è¡Œå…¨ç½‘æƒ…æŠ¥æ£€ç´¢ã€‚\n- åˆ†æè€—æ—¶é¢„è®¡ 45-60 ç§’ã€‚")
                
            with gr.Column(scale=2):
                with gr.Tabs():
                    with gr.Tab("ğŸ“ ç ”æŠ¥è§†å›¾"):
                        out_md = gr.Markdown("ç­‰å¾…åˆ†æ...")
                    with gr.Tab("ğŸ“Š åŸå§‹æ•°æ®"):
                        out_json = gr.JSON()
        
        btn.click(fn=run_analysis, inputs=pdf_input, outputs=[out_md, out_json, out_file], api_name=False)
    
    demo.launch(server_port=8081, inbrowser=True)

if __name__ == "__main__":
    main()
